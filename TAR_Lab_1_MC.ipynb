{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uxpjhFwiIL84"
      },
      "source": [
        "<CENTER>\n",
        "</br>\n",
        "<p><font size=\"5\"> TAR: Taller de Aprendizaje por Refuerzo 2025</span></p>\n",
        "<p><font size=\"5\">  LAB 1: MDP</font></p>\n",
        "</p></br>\n",
        "</p>\n",
        "</CENTER>\n",
        "\n",
        "\n",
        "----------------------------"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OIc5eCXlJajK"
      },
      "source": [
        "In this lab we are going to go through the fundamental concepts of MDP, state and solve a problem for optimal machine replacement, relying on the algorithms seen in class, and no particular MDP python library other than regular ones such as numpy, random, matplotlib."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hs_Q9FjIIL85"
      },
      "source": [
        "## Machine replacement\n",
        "\n",
        "A machine has possible states in $\\{1,\\ldots,S\\}$ that represent how good it performs (the higher the better), state 1 can be consider as broken, state S as new. At each instant $k=0,\\ldots$ either the machine is replaced (action $a=1$) or it is left unchanged (action $a=2$). We assume that\n",
        "\n",
        "\\begin{equation}\n",
        "\\begin{array}{ll}\n",
        "P(s_{k+1}=S|a_k=1) &=1\\\\\n",
        "P(s_{k+1}=s|s_k=s\\_ ,a_k=2) &=(1-\\theta)\\delta_{s\\_ ,s}+\\theta \\delta_{\\max(1,s\\_-1 ),s}.\n",
        "\\end{array}\n",
        "\\end{equation}\n",
        "\n",
        "where $\\delta_{a,b}=1$ if $a=b$ and $0$ otherwise.\n",
        "\n",
        "**1)** Explain the meaning of these probabilities.\n",
        "\n",
        "**2)** Give the expression of transition matrices $P(1)$ and $P(2)$ corresponding to actions $a_k=1$ and $a_k=2$.\n",
        "\n",
        "At times $k=0,\\ldots$, the cost of machine replacement is $c_k(s,a=1)=C$, while if the machine is not replaced, there is still a cost given by $c_k(s,a=2)=(S-s)/(S-1)$, due to the loss of quantity/quality of the production of the machine when its state degrades.\n",
        "\n",
        "**3)** Implement the trajectory of the MDP for a given policy $\\pi$ in a function *trajectory(pi=$\\pi$)* that outputs the trajectory of the state $s$, the trajectory of the action $a$ and resulting cumulative cost $J$.\n",
        "\n",
        "**4)** Implement a plotting function  *plot_trajectory(pi=$\\pi$,policy='')* that runs the *trajectory* function for policy $\\pi$ and plots the trajectories of $s$ and $a$ and gives the value of cumulative cost $J$. The policy string represents the legend in the plot.\n",
        "\n",
        "**5)** Express Bellman's value iteration algorithm. Implement the algorithm for $S=100$, $\\theta=.5$, $C=100$ and discount factor $\\gamma$ as a parameter.  \n",
        "\n",
        "**6)** For the optimal policy, plot a few trajectories of the MDP. Test the influence of parameters $\\theta$, $C$, $\\gamma$... Comments?\n",
        "\n",
        "\n",
        "------------"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "1) \n",
        "La ecuación $ P(s_{k+1}=S|a_k=1) =1 $ implica que al tomar la acción $a=1$ en el instante $k$ el próximo estado es $S$ con una probabilidad de 1. Es decir, la máquina pasa a estado _nuevo_\n",
        "\n",
        "Por otro lado, la ecuación $ P(s_{k+1}=s|s_k=s\\_ ,a_k=2) =(1-\\theta)\\delta_{s\\_ ,s}+\\theta \\delta_{\\max(1,s\\_-1 ),s} $ se puede descomponer en sus dos términos. El primer término refiere a la probabilidad de mantenerse en el mismo estado, es decir $s\\_ = s$, lo cual sucede con una probabilidad $1-\\theta$. El segundo término es la probabilidad de cambiar de estado, en este caso a uno más roto, hasta llegar al 1 que es el más roto, esto ocurre con probabilidad $\\theta$. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "2)\n",
        "La accion 1 lleva de forma deterministica el estado $s_k$ al estado $S$, independientemente del valor de $s_k$. Por lo tanto:\n",
        "\n",
        "$$\n",
        "P(1) = \\begin{bmatrix}\n",
        "0 & 0 & \\cdots & 0 & 1 \\\\\n",
        "0 & 0 & \\cdots & 0 & 1 \\\\\n",
        "\\vdots & \\vdots & \\ddots & \\vdots & \\vdots \\\\\n",
        "0 & 0 & \\cdots & 0 & 1 \\\\\n",
        "0 & 0 & \\cdots & 0 & 1\n",
        "\\end{bmatrix}_{S \\times S}\n",
        "$$\n",
        "\n",
        "Para la accion 2, hay una probabilidad $1-\\theta$ de mantenerse en el mismo estado y $\\theta$ de ir a un estado uno menor. En el caso del estado 1 solo se va a si mismo.\n",
        "\n",
        "$$\n",
        "P(2) = \\begin{bmatrix}\n",
        "1 & 0 & \\cdots & 0 & 0 \\\\\n",
        "\\theta & 1-\\theta & \\cdots & 0 & 0 \\\\\n",
        "\\vdots & \\vdots & \\ddots & \\vdots & \\vdots \\\\\n",
        "0 & 0 & \\cdots & 1-\\theta & 0 \\\\\n",
        "0 & 0 & \\cdots & \\theta & 1-\\theta\n",
        "\\end{bmatrix}_{S \\times S}\n",
        "$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "3."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from typing import  Callable\n",
        "import numpy as np\n",
        "\n",
        "s0 = 10\n",
        "T = 100\n",
        "S = 100\n",
        "theta = 0.5\n",
        "pi = np.ones(S, dtype=int) + 1\n",
        "pi[1] = 1 \n",
        "C = 100\n",
        "\n",
        "def trajectory(pi : Callable, s0: int, T: int, S: int, theta: float, C: float):\n",
        "    \"\"\"\n",
        "\n",
        "    Args:\n",
        "        pi (Callable): Funcion de politica, un mapeo de estados a acciones.\n",
        "                       Recibe uno de {1, ..., S} y devuelve una accion en {1, 2}.\n",
        "                       Forma (S,) donde S es el numero de estados.\n",
        "        s0 (int): Estado inicial\n",
        "        T (int): Cantidad de pasos a simular\n",
        "        S (int): Cantidad de estados\n",
        "        theta (float): Probabilidad de degradacion del estado\n",
        "        C (float): Costo de reemplazo de maquina\n",
        "    Returns:\n",
        "        tuple: Trayectoria de estados, trayectoria de acciones, costo acumulado\n",
        "    \"\"\"\n",
        "    assert pi.shape == (S,)\n",
        "    assert pi.dtype == int\n",
        "    \n",
        "    assert np.all(np.isin(pi, [1, 2]))\n",
        "\n",
        "    s = np.zeros(T, dtype=int)\n",
        "    a = np.zeros(T-1, dtype=int)\n",
        "    J = 0\n",
        "    s[0] = s0\n",
        "    for t in range(T-1):\n",
        "        a[t] = pi[s[t]-1]\n",
        "        if a[t] == 1:\n",
        "            s[t+1] = S\n",
        "            J += C\n",
        "        elif a[t] == 2:\n",
        "            if s[t] == 1:\n",
        "                s[t+1] = 1\n",
        "            else:\n",
        "                choices = np.array([s[t]-1, s[t]])\n",
        "                probs = np.array([theta, 1-theta])\n",
        "                s[t+1] = np.random.choice(choices, p=probs)\n",
        "                J += (S-s[t])/(S-1)\n",
        "        else:\n",
        "            raise ValueError(f\"Invalid action: {a[t]}\")\n",
        "        \n",
        "    return s, a, J\n",
        "\n",
        "s,a, j = trajectory(pi=pi, s0=s0, T=T, S=S, theta=theta, C=C)\n",
        "print(f\"State trajectory: {s}\")\n",
        "print(f\"Action trajectory: {a}\")\n",
        "print(f\"Cumulative cost: {j}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "4."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "def plot_trajectories(pi : np.ndarray, policy: str, T: int, S: int, theta: float, C: float):\n",
        "    \"\"\"Grafica las trayectorias de estado y accion para una politica dada.\n",
        "\n",
        "    Args:\n",
        "        pi (np.ndarray): Politica que mapea estados a acciones.\n",
        "                        Forma (S+1,) donde S es el numero de estados.\n",
        "                        Mapea {1, ..., S} a {1, 2}\n",
        "        policy (str): Nombre de la politica para etiquetar el grafico.\n",
        "        T (int): Cantidad de pasos a simular.\n",
        "        S (int): Cantidad de estados.\n",
        "        theta (float): Probabilidad de degradacion del estado.\n",
        "        C (float): Costo de reemplazo de la maquina.\n",
        "    \"\"\"    \n",
        "    s, a, J = trajectory(pi, s0, T, S, theta, C)\n",
        "    print(f\"El costo acumulado para la politica {policy} es {J}\")\n",
        "        \n",
        "    fig = plt.figure(figsize=(10, 6))\n",
        "    plt.plot(s, label='Estado')\n",
        "    plt.plot(a, label='Accion')\n",
        "    plt.legend()\n",
        "    plt.title(f'Politica {policy}')\n",
        "    plt.ylabel('Estado/Accion')\n",
        "    plt.xlabel('Step')\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.close()\n",
        "    return fig\n",
        "    \n",
        "plot_trajectories(pi = pi, policy='Reemplazar en estado 1', T=T, S=S, theta=theta, C=C)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "5."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def value_iteration(S, theta, C, gamma, max_iter=1000, tol=1e-6):\n",
        "    \"\"\"\n",
        "    Implementa el algoritmo de iteracion de valor de Bellman para el problema de reemplazo de maquinas.\n",
        "    \n",
        "    Parametros:\n",
        "    -----------\n",
        "    S : int\n",
        "        Numero de estados posibles para la maquina (1 a S).\n",
        "    theta : float\n",
        "        Parametro de probabilidad para la transicion de estado cuando la maquina no se reemplaza.\n",
        "    C : float\n",
        "        Costo de reemplazo de la maquina.\n",
        "    gamma : float\n",
        "        Factor de descuento para costos futuros.\n",
        "    max_iter : int\n",
        "        Numero maximo de iteraciones.\n",
        "    tol : float\n",
        "        Tolerancia para la convergencia.\n",
        "    Retorna:\n",
        "    --------\n",
        "    V : ndarray\n",
        "        Funcion de valor optima para cada estado.\n",
        "    pi : ndarray\n",
        "        Politica optima para cada estado (1 = reemplazar, 2 = mantener).\n",
        "    \"\"\"\n",
        "    V = np.zeros(S)\n",
        "    pi = np.zeros(S, dtype=int)\n",
        "\n",
        "    for _ in range(max_iter):\n",
        "        for s in range(S):\n",
        "            # Calculamos la reward como menos el costo\n",
        "            r = [-C, -(S - (s + 1)) / (S - 1)]\n",
        "            # a = 1:\n",
        "            V_new_a1 = r[0] + gamma * V[S-1]\n",
        "            # a = 2:\n",
        "            V_new_a2 = ((1-theta) * (r[1] + gamma * V[s])) + (theta * (r[1] + gamma * V[max(1, s - 1)]))\n",
        "\n",
        "            V_old = V[s]\n",
        "            V[s] = max(V_new_a1, V_new_a2)\n",
        "\n",
        "            pi[s] = np.argmax([V_new_a1, V_new_a2]) + 1\n",
        "            if np.abs(V_old - V[s]) < tol:\n",
        "                break\n",
        "\n",
        "    return V, pi\n",
        "\n",
        "gamma = 0.9\n",
        "\n",
        "V, pi = value_iteration(S=S, theta=theta, C=C, gamma=gamma)\n",
        "\n",
        "plot_trajectories(pi = pi, policy='Optimal', T=T, S=S, theta=theta, C=C)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Se ve como la política óptima en el caso donde el costo es $C=100$ es siempre mantener la máquina y nunca reemplazar. Esto se debe a que $c_k(s,a=1) >> 1 \\geq c_k(s,a=1)$. Como se busca minimizar el costo (o maximizar menos el costo) la política óptima resulta en nunca reemplazar la máquina."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "6"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "gammas = [0.1, 0.5, 0.9]\n",
        "Cs = [0.01, 0.1, 1, 5]\n",
        "S = 100\n",
        "T = 100\n",
        "theta = 0.5\n",
        "\n",
        "Vs = []\n",
        "pis = []\n",
        "policies = []\n",
        "figures = []\n",
        "\n",
        "fig, axs = plt.subplots(3, 4, figsize=(20, 15))\n",
        "fig.tight_layout(pad=3.0)\n",
        "\n",
        "for i, C in enumerate(Cs):\n",
        "    for j, gamma in enumerate(gammas):    \n",
        "        V, pi = value_iteration(S=S, theta=theta, C=C, gamma=gamma)\n",
        "        Vs.append(V)\n",
        "        pis.append(pi)\n",
        "        policies.append(f\"C={C} gamma={gamma}\")\n",
        "        \n",
        "        ax = axs[j, i]\n",
        "        subfig = plot_trajectories(pi=pi, policy=f'C={C} gamma={gamma}', T=T, S=S, theta=theta, C=C)\n",
        "        ax.clear()\n",
        "        for line in subfig.axes[0].get_lines():\n",
        "            ax.plot(line.get_xdata(), line.get_ydata(), color=line.get_color())\n",
        "        ax.set_title(f'C={C}, gamma={gamma}')\n",
        "        ax.set_xlabel('Time')\n",
        "        ax.set_ylabel('State')\n",
        "        ax.grid(True)\n",
        "\n",
        "plt.subplots_adjust(hspace=0.3, wspace=0.3)\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "A medida que el valor de $C$ aumenta, se observa cómo la máquina es reemplazada cada vez menos veces. Esto ocurre porque el costo de reemplazo se vuelve alto en comparación con el costo de mantenimiento. Cuando el costo de reemplazo es bajo, la máquina tiende a reemplazarse más seguido para incurrir en un menor costo por la degradación de la misma, ya que resulta más económico obtener una máquina nueva que mantener una deteriorada.\n",
        "\n",
        "Por otro lado, cuando $\\gamma$ es bajo, las decisiones se toman en base al corto plazo, sin considerar mucho las consecuencias futuras. Este comportamiento se observa claramente en los casos de $C=0.1$ y $C=1$. Para valores pequeños de $\\gamma$, las decisiones se toman ponderando únicamente unos pocos estados hacia adelante, conduciendo a decisiones más _greedy_, es decir, tomando la mejor decisión paso a paso e ignorando el impacto a largo plazo. Tanto para $C=0.1$ como para $C=1$ se observa que la máquina se reemplaza más seguido cuando $\\gamma=0.9$, ya que el agente considera el futuro con mayor importancia y reconoce que a largo plazo el costo acumulado de no reemplazar será cada vez más alto debido a la degradación."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "gamma = 0.9\n",
        "Cs = [0.01, 0.1, 1, 5]\n",
        "S = 100\n",
        "T = 200\n",
        "thetas = [0.1, 0.5, 0.9, 1]\n",
        "\n",
        "Vs = []\n",
        "pis = []\n",
        "policies = []\n",
        "figures = []\n",
        "\n",
        "fig, axs = plt.subplots(4, 4, figsize=(20, 15))\n",
        "fig.tight_layout(pad=3.0)\n",
        "\n",
        "for i, C in enumerate(Cs):\n",
        "    for j, theta in enumerate(thetas):    \n",
        "        V, pi = value_iteration(S=S, theta=theta, C=C, gamma=gamma)\n",
        "        Vs.append(V)\n",
        "        pis.append(pi)\n",
        "        policies.append(f\"C={C} theta={theta}\")\n",
        "        \n",
        "        ax = axs[i, j]\n",
        "        subfig = plot_trajectories(pi=pi, policy=f'C={C} theta={theta}', T=T, S=S, theta=theta, C=C)\n",
        "        ax.clear()\n",
        "        for line in subfig.axes[0].get_lines():\n",
        "            ax.plot(line.get_xdata(), line.get_ydata(), color=line.get_color())\n",
        "        ax.set_title(f'C={C}, theta={theta}')\n",
        "        ax.set_xlabel('Time')\n",
        "        ax.set_ylabel('State')\n",
        "        ax.grid(True)\n",
        "\n",
        "plt.subplots_adjust(hspace=0.3, wspace=0.3)\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Observando las gráficas, se puede notar que la trayectoria óptima parece no cambiar significativamente para distintos valores de $\\theta$ cuando se mantiene el mismo valor de $C$. Esto tiene sentido, ya que $\\theta$ solo determina la velocidad a la que se deteriora la máquina (la probabilidad de transición entre estados), pero no afecta directamente los costos del problema.\n",
        "\n",
        " Sin embargo, lo que sí cambia con diferentes valores de $\\theta$ es la frecuencia con la que la máquina alcanza estados deteriorados. Con valores más altos de $\\theta$, la máquina se deteriora más rápidamente, lo que lleva a que se alcancen los umbrales de reemplazo en menos tiempo. Esto se puede observar en las gráficas donde, para un mismo valor de $C$, las trayectorias con $\\theta$ más alto muestran ciclos de más alta frecuencia.\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
