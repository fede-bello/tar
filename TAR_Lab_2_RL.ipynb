{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kCmjRktM7Wvo"
      },
      "source": [
        "<CENTER>\n",
        "</br>\n",
        "<p><font size=\"5\"> TAR: Taller de Aprendizaje por Refuerzo 2025</span></p>\n",
        "<p><font size=\"5\">  LAB 2: RL</font></p>\n",
        "</p></br>\n",
        "</p>\n",
        "</CENTER>\n",
        "\n",
        "\n",
        "----------------------------"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p-Zp_fsz7Wvq"
      },
      "source": [
        "## Machine replacement - Continuation of Lab 1\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YGL82KC7nsiS"
      },
      "source": [
        "In previous lab, we have computed the optimal policy using Dynamic Programming solutions In this lab, we focus on implicit simulation-based algorithms for solving MDPs, called reinforcement learning algorithms. We will study two algorithms to solve the Machine replacement problem. The first algorithm studied here is the famous Q-learning algorithm. The second one will use the particular stucture of the optimal policy of Machine replacement problem.\n",
        "\n",
        "**Remark:** There are two main motivations behind the use of simulation based algorithms: (1) Pratical scenario where we don't know the transition probability matrix and we only have access to an oracle which will generate the event $p(i\\mid j)$ (simulation codes in aeronotics/ or predicting human behavior, for instance). (2) These methods are efficient and fast because  the algorithm is not wasted time to have a good estimation of parameters which are rarely visited by the simulator.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "**1)** Implement of the function *Q_learning($T,s_0,a_0,S,\\theta,C,\\gamma,\\beta$)* which returns the Q-factors at each time ($[[[Q_k(s,a)]]]_{1\\leq k\\leq T,a\\in A, s\\in S}$) and the sequence of strategy $[a_k]_{1\\leq k\\leq T}$ of the Q-learning algorithm, where one iteration $(s,k)$ is equal to:\n",
        "$$\n",
        "\\begin{array}{lll}\n",
        "s_{k+1}&\\sim&p(\\cdot\\mid s_k,a_k=a),\\\\\n",
        "Q_{k+1}(s,a)&=&Q_{k}(s,a)\\\\\n",
        "&&+\\alpha \\times 1_{s_{k}=s,a_k=a}\\left(c_k(s,a)+\\gamma\\min_{b}Q_{k}(s_{k+1},b)-Q_k(s,a)\\right),\\\\\n",
        "a_{k+1}&=&\\left\\{\\begin{array}{lll}\n",
        "a\\sim Uniform&\\text{with probability}&\\epsilon,\\\\\n",
        "\\text{arg min }_b  Q_{k+1}(s_{k+1},b)&\\text{with probability}& 1-\\epsilon. \\end{array}\\right.\\\\\n",
        "\\end{array}\n",
        "$$\n",
        "\n",
        "The starting state is equal to $s_0$. Note here that we are minimizing a discounted cumulative cost and not, as presented during the class, maximizing a discounted cumulative reward. This change, in the Q-learning algorithm, is simply translated into the use of the min operator instead of the max operator.\n",
        "\n",
        "For the following question, we assume that $S=10$, i.e, the number of states is equal to 10.\n",
        "\n",
        "**2)** Simulate the Q-learning algorithm for the replacement problem, until convergence (Stop when $\\max_{s,a}\\mid Q_{k+1}(s,a)- Q_{k}(s,a)\\mid< 0.01 $, you can also try with $0.001$).\n",
        "\n",
        "**2a**) Plot the trajectory of $\\min_b\\{Q_k(s,b)\\}$ and the trajectory of the value function obtained through the value iteration algorithm previously implemented.\n",
        "\n",
        "Please answer the following questions based on your observed results:\n",
        "\n",
        "**2b)** You should observe that the Q-learning tracks the trajectory of the value function, why?\n",
        "\n",
        "**2c)** You should observe that the variance of the Q-learning algorithm (the variance around the true value) is decreasing over the number of iterations. After how many iterations the Q-learning is stabilizing?\n",
        "\n",
        "**2d)** If the Q-learning algorithm stops at k=10, how good/bad the obtained policy is?\n",
        "\n",
        "**3)** Plot the trajectory of $a_k$ for the Q-learning algorithm and for the value iteration algorithm.\n",
        "\n",
        "**3a**) How does the distance between the two quantities ($\\sum_{s=1}^S\\mid a_k^{\\text{Q-learning}}(s)-a_k^{\\text{Value Iteration}}(s)\\mid$) evolve?\n",
        "\n",
        "**3b)** After how many iterations the policy generated by the Q-learning could be implemented in a real scenario, i.e. could be considered good enough?\n",
        "\n",
        "**3c)** Replace $\\alpha$ by $\\frac{1}{k}$ and $\\epsilon$ by $\\frac{\\epsilon}{k^{2/3}}$ and run the QL algroithm again. You should observe that the Q-learning algorithm with the decreasing step-size is converging faster than the Q-learning algorithm with the constant step-size, why? How would your answer to 2d) change now?\n",
        "\n",
        "**4)** In Part I of this lab, you must have observed that the optimal policy has a threshold shape: $\\pi^*(s)=1_{s<s^*}+2(1-1_{s<s^*})$. Our next algorithm is based on this fact.\n",
        "\n",
        "**4a)** For each $s^*\\in S$ simulate 10 times the discounted cumulative cost (using *simulation($s^*$e,a,S)*), when $\\pi^*(s)=1_{s<s^*}+2(1-1_{s<s^*})$.\n",
        "\n",
        "**4b)** Compute the mean of the simulated discounted cost for each policy and deduce the best threshold.  Compare with the Q-learning approach. Which algorithm is the best?\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "------------"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "IexpYoQ4NbdQ"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}